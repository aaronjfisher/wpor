% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuneflow.R
\name{tune_wf}
\alias{tune_wf}
\alias{as.tuneflow}
\alias{fit.tuneflow}
\title{Tune a workflow and return the best workflow}
\usage{
tune_wf(
  wf,
  data = NULL,
  v = 10,
  size = 10,
  resamples = rsample::vfold_cv(data, v),
  alpha = 0.05,
  burnin = length(resamples$splits),
  verbose = TRUE,
  save_performance = FALSE
)

as.tuneflow(wf, ...)

\method{fit}{tuneflow}(object, data)
}
\arguments{
\item{wf}{a parsnip workflow}

\item{data}{a dataset to train on}

\item{v}{number of cv folds to use in resamples; overwritten by resamples, if provided.}

\item{size}{number of param sets to evaluate}

\item{resamples}{An ‘rset()’ object object used to evaluate wf}

\item{burnin}{how many folds to examine before discarding
poorly performing parameter sets}

\item{save_performance}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}} save performance
metrics as an attribute. If using as.tuneflow, this attribute
will also be saved in the final, fitted workflow.}

\item{...}{args to be passed to tune_wf (e.g., v, grid) before fitting the workflow.}
}
\description{
\code{tune_wf}, \code{as.tuneflow} and \code{fit.tuneflow} are convenience functions
for tuning a workflow.

fit.tuneflow returns a tuned, fitted workflow.
}
\details{
\code{tune_wf} finalizes a workflow using \code{finetune::tune_race_anova},
cross-validation, and either RMSE
or AUC, depending on the workflow's \code{mode}.

\code{as.tuneflow} creates an object of class \code{tuneflow}, which is a list containing
(1) a workflow, and (2) instructions on how to tune it, i.e., arguments to be
passed to tune_wf.

At the time of fitting, \code{fit.tuneflow} first applies tune_wf
to finalize the workflow, using the stored arguments from \code{as.tuneflow},
and then applies \code{fit.workflow} to fit the finalized workflow.
}
\examples{
\dontrun{
library(tidymodels)
library(dplyr)

set.seed(0)
train_data <- sim_data(setup = "A", n = 300, p = 6, sigma = 1)$data

x_terms <- train_data \%>\%
  select(starts_with("x.")) \%>\%
  colnames()
rhs <- paste(x_terms, collapse = " + ")
treatment_formula <- formula(paste("treatment ~", rhs))

mod_spec <- boost_tree(min_n = tune(), learn_rate = tune()) \%>\%
  set_engine("xgboost")

wf <- workflow() \%>\%
  add_model(set_mode(mod_spec, "classification")) \%>\%
  add_formula(treatment_formula)

if (FALSE) {
  # Optionally, the tuning process can be parallelized by first
  # registering a cluster.
  cores <- parallel::detectCores(logical = FALSE)
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)
}

## Example using tune_wf explicitly
set.seed(0)
tuned_wf <- tune_wf(wf, data = train_data)
fitted1 <- fit(tuned_wf, train_data)
pred1 <- predict(fitted1, train_data, "prob")

## Example using tune_wf implicitly
set.seed(0)
fitted2 <- as.tuneflow(wf) \%>\%
  wpor::fit(train_data)
pred2 <- predict(fitted2, train_data, "prob")

range(pred1 - pred2) # equivalent results
}
}
