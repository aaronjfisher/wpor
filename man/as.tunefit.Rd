% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tunefit.R
\name{tune_params}
\alias{tune_params}
\alias{as.tunefit}
\alias{fit.tunefit}
\title{Tune a workflow or training algorithm}
\usage{
tune_params(
  trainer,
  grid = NULL,
  metric = NULL,
  data = NULL,
  v = 10,
  resamples = rsample::vfold_cv(data, v),
  alpha = 0.05,
  burnin = length(resamples$splits),
  verbose = FALSE,
  save_performance = FALSE,
  size = 10
)

as.tunefit(trainer, ...)

\method{fit}{tunefit}(object, data)
}
\arguments{
\item{trainer}{a parsnip workflow or model specification that can be tuned.}

\item{data}{a dataset to train on}

\item{v}{number of cv folds to use in resamples; overwritten by resamples, if provided.}

\item{resamples}{An ‘rset()’ object object used to evaluate wf}

\item{burnin}{how many folds to examine before discarding
poorly performing parameter sets}

\item{save_performance}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}} save performance
metrics as an attribute. If using as.tunefit, this attribute
will also be saved in the final, fitted workflow.}

\item{size}{number of param sets to evaluate}

\item{...}{args to be passed to tune_params (e.g., v, grid) before fitting the workflow.}
}
\value{
a trainer or workflow with parameters updated to be the
parameters discovered to have the lowest cross-validated error.
}
\description{
\code{tune_params}, \code{as.tunefit} and \code{fit.tunefit} are convenience functions
for tuning a workflow.

fit.tunefit returns a tuned, fitted workflow.
}
\details{
\code{tune_params} finalizes a workflow using \code{finetune::tune_race_anova},
cross-validation, and either RMSE
or AUC, depending on the workflow's \code{mode}.

\code{as.tunefit} creates an object of class \code{tunefit}, which is a list containing
(1) a workflow, and (2) instructions on how to tune it, i.e., arguments to be
passed to tune_params.

At the time of fitting, \code{fit.tunefit} first applies tune_params
to finalize the workflow, using the stored arguments from \code{as.tunefit},
and then applies \code{fit.workflow} to fit the finalized workflow.
}
\examples{
\dontrun{
library(tidymodels)
library(dplyr)

set.seed(0)
train_data <- sim_data(setup = "A", n = 300, p = 6, sigma = 1)$data

x_terms <- train_data \%>\%
  select(starts_with("x.")) \%>\%
  colnames()
rhs <- paste(x_terms, collapse = " + ")
treatment_formula <- formula(paste("treatment ~", rhs))

mod_spec <- boost_tree(min_n = tune(), learn_rate = tune()) \%>\%
  set_engine("xgboost")

wf <- workflow() \%>\%
  add_model(set_mode(mod_spec, "classification")) \%>\%
  add_formula(treatment_formula)

if (FALSE) {
  # Optionally, the tuning process can be parallelized by first
  # registering a cluster.
  cores <- parallel::detectCores(logical = FALSE)
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)
}

## Example using tune_params explicitly
set.seed(0)
tuned_wf <- tune_params(wf, data = train_data)
fitted1 <- fit(tuned_wf, train_data)
pred1 <- predict(fitted1, train_data, "prob")

## Example using tune_params implicitly
set.seed(0)
fitted2 <- as.tunefit(wf) \%>\%
  wpor::fit(train_data)
pred2 <- predict(fitted2, train_data, "prob")

range(pred1 - pred2) # equivalent results
}
}
