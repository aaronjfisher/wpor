---
title: "Parameter Tuning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Parameter Tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---




```{r, include = FALSE}
# Simulate data
library("wpor")
library("dplyr")


set.seed(0)
training <- sim_data(setup = "A", n = 700, p = 6, sigma = 1)
tdata <- training$data
tdata$w <- 1

gbm <- lightgbm_spec(
  formula = training$formulas$treatment,
  mode = "classification",
)
gbm

head(get_y(gbm, tdata))
head(get_x(gbm, tdata))
update_params(gbm, list(learning_rate = .1))


gbm_grid <- lightgbm_grid(30)

tuned_gbm <- tune_params(
  gbm,
  grid = gbm_grid,
  metric = NULL,
  resamples = rsample::vfold_cv(tdata, 10),
  alpha = 0.05,
  burnin = 3,
  verbose = TRUE,
  save_performance = TRUE
)
matplot(attributes(tuned_gbm)$tune_results$performance, type = "l")


fit_tuned_gbm <- fit(tuned_gbm, tdata)
fit_untuned_gbm <- fit(gbm, tdata)

test <- sim_data(setup = "A", n = 2000, p = 6, sigma = 1)
boxplot(predict(fit_tuned_gbm, test$data) ~ test$data$treatment)
boxplot(predict(fit_untuned_gbm, test$data) ~ test$data$treatment)





library(tidymodels)
mod_spec <- boost_tree(min_n = tune(), learn_rate = tune()) %>%
  set_engine("xgboost")
#'
wf <- workflow() %>%
  add_model(set_mode(mod_spec, "classification")) %>%
  add_formula(training$formulas$treatment)


tuned_wf <- tune_params(
  wf,
  metric = NULL,
  resamples = rsample::vfold_cv(tdata, 10),
  alpha = 0.05,
  burnin = 3,
  verbose = TRUE,
  save_performance = TRUE
)

test <- sim_data(setup = "A", n = 2000, p = 6, sigma = 1)
fit_tuned_wf <- fit(tuned_wf, tdata)
boxplot(predict_expected_value(fit_tuned_wf, test$data) ~ test$data$treatment)
```
